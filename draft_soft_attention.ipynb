{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from datasets.bdd_oia import BDD_OIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_home = './data/bdd_oia/lastframe/data/'\n",
    "label_home = './data/bdd_oia/lastframe/labels/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    file_name              reason        action  \\\n",
       "2075  b82609d8-eef37c99_3.jpg  [0, 1, 0, 1, 0, 0]  [1, 0, 0, 0]   \n",
       "2078  af1425f4-892440e1_3.jpg  [0, 1, 0, 0, 0, 0]  [1, 0, 0, 1]   \n",
       "2365    047e732b-aa79a87d.jpg  [0, 1, 0, 0, 1, 0]  [1, 0, 0, 0]   \n",
       "\n",
       "                                         reason_lang  \\\n",
       "2075  no lane on the left;obstacles on the rightlane   \n",
       "2078                             no lane on the left   \n",
       "2365        no lane on the left;no lane on the right   \n",
       "\n",
       "                                 reason_lang_ind  \n",
       "2075     [3, 7, 4, 9, 13, 5, 1, 8, 9, 13, 11, 2]  \n",
       "2078                      [3, 7, 4, 9, 13, 5, 2]  \n",
       "2365  [3, 7, 4, 9, 13, 5, 1, 7, 4, 9, 13, 10, 2]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>reason</th>\n      <th>action</th>\n      <th>reason_lang</th>\n      <th>reason_lang_ind</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2075</th>\n      <td>b82609d8-eef37c99_3.jpg</td>\n      <td>[0, 1, 0, 1, 0, 0]</td>\n      <td>[1, 0, 0, 0]</td>\n      <td>no lane on the left;obstacles on the rightlane</td>\n      <td>[3, 7, 4, 9, 13, 5, 1, 8, 9, 13, 11, 2]</td>\n    </tr>\n    <tr>\n      <th>2078</th>\n      <td>af1425f4-892440e1_3.jpg</td>\n      <td>[0, 1, 0, 0, 0, 0]</td>\n      <td>[1, 0, 0, 1]</td>\n      <td>no lane on the left</td>\n      <td>[3, 7, 4, 9, 13, 5, 2]</td>\n    </tr>\n    <tr>\n      <th>2365</th>\n      <td>047e732b-aa79a87d.jpg</td>\n      <td>[0, 1, 0, 0, 1, 0]</td>\n      <td>[1, 0, 0, 0]</td>\n      <td>no lane on the left;no lane on the right</td>\n      <td>[3, 7, 4, 9, 13, 5, 1, 7, 4, 9, 13, 10, 2]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# labels = pd.read_csv(label_home+'no_train.csv')\n",
    "labels = pd.read_pickle(label_home+'no_train.pkl')\n",
    "labels.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_name = labels.iloc[0]['file_name']\n",
    "test_img = Image.open(image_home+test_img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "mean=[102.9801, 115.9465, 122.7717]\n",
    "std=[1., 1., 1.]\n",
    "\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize(mean,std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of samples in dataset:19936\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 180, 320])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "class BDD_OIA_NLP(Dataset):\n",
    "    def __init__(self, image_root, label_root, ind_to_word_root, image_min_size=120):\n",
    "        super().__init__()\n",
    "        import torchvision.transforms as T\n",
    "\n",
    "        self.image_root = image_root\n",
    "        self.mean=[102.9801, 115.9465, 122.7717]\n",
    "        self.std=[1., 1., 1.]\n",
    "\n",
    "        self._processing(label_root,ind_to_word_root)\n",
    "\n",
    "        self.transform = T.Compose([T.Resize(image_min_size),\n",
    "                                    T.ToTensor(),\n",
    "                                #    T.Normalize(self.mean,self.std),\n",
    "                                    ])\n",
    "\n",
    "    def _processing(self, label_root, ind_to_word_root):\n",
    "        data_df = pd.read_pickle(label_root)\n",
    "\n",
    "        self.count = len(data_df)\n",
    "        self.all_images = data_df['file_name']\n",
    "        self.all_reasons = data_df['reason_lang_ind']\n",
    "        self.all_actions = data_df['action']\n",
    "        print(\"number of samples in dataset:{}\".format(self.count))\n",
    "\n",
    "        with open(ind_to_word_root,'rb') as f:\n",
    "            self.ind_to_word = pickle.load(f)\n",
    "\n",
    "        self.word_to_ind = dict([(value, key) for key, value in self.ind_to_word.items()]) \n",
    "\n",
    "        self.num_words = len(self.ind_to_word.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # test = True\n",
    "        target = {}\n",
    "        image_name = self.all_images.iloc[idx]\n",
    "\n",
    "        # print(type(self.all_actions[idx]))\n",
    "        target['action'] = torch.tensor(self.all_actions.iloc[idx][:4])\n",
    "        target['reason'] = torch.tensor(self.all_reasons.iloc[idx])\n",
    "\n",
    "        img_ = Image.open(self.image_root + image_name)\n",
    "\n",
    "        img = self.transform(img_)\n",
    "\n",
    "        return img,target\n",
    "\n",
    "image_dir = './data/bdd_oia/lastframe/data/'\n",
    "label_dir = './data/bdd_oia/lastframe/labels/'\n",
    "bdd_oia_dataset = BDD_OIA_NLP(image_dir, label_dir+'no_train.pkl', label_dir+'ind_to_word.pkl',image_min_size=180)\n",
    "\n",
    "training_loader = DataLoader(bdd_oia_dataset,\n",
    "                            shuffle=True,\n",
    "                            batch_size=10,\n",
    "                            num_workers=0,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=utils.collate_fn)\n",
    "images_batch,labels_batch = next(iter(training_loader))\n",
    "images_batch = torch.stack(images_batch)\n",
    "images_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "mobilenet = torchvision.models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# summary(resnet,(3,180,320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_mobilenet = nn.Sequential(*list(mobilenet.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 1280, 6, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "image_feature = feature_extractor_mobilenet(images_batch)\n",
    "image_feature.shape"
   ]
  },
  {
   "source": [
    "## Language Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([ 3,  8,  9, 13,  6,  1,  8,  9, 13, 11,  2]),\n",
       " tensor([ 3,  8,  9, 13,  6,  1,  8,  9, 13, 11,  2]),\n",
       " tensor([ 3,  8,  9, 13,  6,  1,  8,  9, 13, 11,  2]),\n",
       " tensor([ 3,  8,  9, 13,  6,  1,  8,  9, 13, 11,  2]),\n",
       " tensor([ 3,  8,  9, 13,  6,  1,  8,  9, 13, 11,  2]),\n",
       " tensor([ 3, 12,  9, 13,  5,  1,  7,  4,  9, 13, 10,  2]),\n",
       " tensor([ 3,  8,  9, 13,  6,  1,  8,  9, 13, 11,  2]),\n",
       " tensor([ 3, 12,  9, 13,  5,  1, 12,  9, 13, 10,  2]),\n",
       " tensor([ 3,  8,  9, 13,  6,  2]),\n",
       " tensor([ 3, 12,  9, 13,  5,  1,  8,  9, 13, 11,  2])]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "reason_batch = [l['reason'] for l in labels_batch]\n",
    "reason_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasonDecoder(nn.Module):\n",
    "    def __init__(self, image_f_dim, embedding_dim, hidden_dim, dict_size, device='cpu', null_index=0, start_index = 3, end_index=2,using_gate=True):\n",
    "        super().__init__()\n",
    "        self._NULL_INDEX = null_index\n",
    "        self._START_INDEX = start_index\n",
    "        self._DICT_SIZE = dict_size\n",
    "        self._END_INDEX = end_index\n",
    "\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._image_f_dim = image_f_dim\n",
    "        \n",
    "        self.using_gate = using_gate\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=self._DICT_SIZE,embedding_dim=self._embedding_dim)\n",
    "        self.image_affine_layers = nn.Sequential(nn.Linear(self._image_f_dim,self._hidden_dim),nn.ReLU()) \n",
    "        self.init_c_layer = nn.Linear(self._hidden_dim,self._hidden_dim)\n",
    "        self.init_h_layer = nn.Linear(self._hidden_dim,self._hidden_dim)\n",
    "        self.lstm_step = nn.LSTMCell(input_size=hidden_dim*2,hidden_size=hidden_dim)\n",
    "        self.score_layer = nn.Linear(hidden_dim, self._DICT_SIZE)      \n",
    "        if self.using_gate:\n",
    "            self.gate = nn.Sequential(nn.Linear(self._hidden_dim, self._hidden_dim), nn.Sigmoid())\n",
    "        self.loss = self.loss_fn()\n",
    "\n",
    "    def loss_fn(self):\n",
    "        reason_loss = nn.CrossEntropyLoss().to(self.device)\n",
    "        return reason_loss\n",
    "\n",
    "    def dot_product_attention(self, prev_h, A):\n",
    "        \"\"\"\n",
    "        dot product between the hidden state and image embeddings\n",
    "        A: B,F,H,W\n",
    "        prev_h: B,F\n",
    "        \n",
    "        (F: hidden dimension)\n",
    "        \"\"\"\n",
    "        B,F,H,W = A.shape\n",
    "        attention_score = torch.bmm(prev_h.view(B,1,F),A.view(B,F,H*W)).squeeze(1).div(F**0.5) # B, H*W\n",
    "        attention_weights = nn.functional.softmax(attention_score,dim=1) # B, H*W\n",
    "        attended_features = torch.bmm(A.view(B,F,H*W), attention_weights.view(B,H*W,1)).squeeze(2) # B, F\n",
    "        attention_weights = attention_weights.view(B,H,W)\n",
    "\n",
    "        return attention_weights, attended_features\n",
    "\n",
    "    def init_hidden_state(self, A):\n",
    "        mean_image = A.mean(dim=(2,3)) # N,image_feature_dim\n",
    "        h0 = self.init_h_layer(mean_image)\n",
    "        c0 = self.init_c_layer(mean_image)\n",
    "        return h0, c0   \n",
    "\n",
    "    def forward(self, image_feature, reason_batch):\n",
    "\n",
    "        image_f = self.image_affine_layers(image_feature.permute(0,2,3,1)).permute(0,3,1,2) # B , F , H , W \n",
    "\n",
    "        ## Manipulate the reason batch\n",
    "        padded = nn.utils.rnn.pad_sequence(reason_batch,padding_value=self._NULL_INDEX)\n",
    "        embedded = self.embedding_layer(padded)\n",
    "        embedded_input = embedded[:-1,:,:]\n",
    "        padded_output = padded[1:,:]\n",
    "        T,B,E = embedded_input.shape\n",
    "        \n",
    "        h,c = self.init_hidden_state(image_f)\n",
    "        \n",
    "        hs = []\n",
    "        scores = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            attention_weight, attended_features = self.dot_product_attention(h,image_f)\n",
    "            # print(attended_feature.shape)\n",
    "            if self.using_gate:\n",
    "                gate_val = self.gate(h)\n",
    "                # print(gate_val.shape)\n",
    "                attended_features = attended_features*gate_val\n",
    "            \n",
    "            lstm_input = torch.cat([attended_features,embedded_input[t]],axis=1)\n",
    "            h,c = self.lstm_step(lstm_input,(h,c))\n",
    "            score = self.score_layer(h)\n",
    "            hs.append(h)\n",
    "            scores.append(score)\n",
    "            attention_weights.append(attention_weight)\n",
    "        hs, scores, attention_weights = torch.stack(hs), torch.stack(scores) ,torch.stack(attention_weights) \n",
    "\n",
    "        if self.training:\n",
    "            loss = self.loss(scores.view(-1,self._DICT_SIZE), padded_output.view(-1))\n",
    "            return loss,scores, attention_weights, hs\n",
    "        else:\n",
    "            return scores, attention_weights, hs #hs: T B F, attention weights: T, B, H, W \n",
    "    \n",
    "    def generate_reason(self, image_feature, max_length=33):\n",
    "        image_f = self.image_affine_layers(image_feature.permute(0,2,3,1)).permute(0,3,1,2) # B , F , H , W \n",
    "\n",
    "        B = image_f.shape[0]\n",
    "        \n",
    "        prev_word = torch.LongTensor([self._START_INDEX]*B).to(self.device)\n",
    "\n",
    "        h0,c0 = self.init_hidden_state(image_f)\n",
    "        h = h0\n",
    "        c = c0\n",
    "        hs = []\n",
    "        scores = []\n",
    "        attention_weights = []\n",
    "        reasons = []\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            prev_embedding = self.embedding_layer(prev_word)\n",
    "            attention_weight, attended_features = self.dot_product_attention(h,image_f)\n",
    "            # print(attended_feature.shape)\n",
    "            if self.using_gate:\n",
    "                gate_val = self.gate(h)\n",
    "                # print(gate_val.shape)\n",
    "                attended_features = attended_features*gate_val\n",
    "            \n",
    "            lstm_input = torch.cat([attended_features,prev_embedding],axis=1)\n",
    "            h,c = self.lstm_step(lstm_input,(h,c))\n",
    "            score = self.score_layer(h)\n",
    "            prev_word = torch.max(score,1)[1]\n",
    "            hs.append(h)\n",
    "            scores.append(score)\n",
    "            attention_weights.append(attention_weight)\n",
    "            reasons.append(prev_word)\n",
    "        hs, scores, attention_weights, reasons = torch.stack(hs), torch.stack(scores),torch.stack(attention_weights),torch.stack(reasons) \n",
    "\n",
    "        return hs, scores, attention_weights, reasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_INDEX = bdd_oia_dataset.word_to_ind['NULL']\n",
    "DICT_SIZE = len(bdd_oia_dataset.word_to_ind.keys())\n",
    "\n",
    "\n",
    "reason_decoder = ReasonDecoder(image_f_dim=1280,\\\n",
    "                               embedding_dim=128, \\\n",
    "                               hidden_dim=128, \\\n",
    "                               dict_size=DICT_SIZE, \\\n",
    "                               device='cpu',\\\n",
    "                               null_index=NULL_INDEX, \\\n",
    "                               using_gate=True)\n",
    "\n",
    "# reason_decoder.train()\n",
    "loss, scores, attention_weights, hs = reason_decoder(image_feature,reason_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([12, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "NULL_INDEX = bdd_oia_dataset.word_to_ind['NULL']\n",
    "DICT_SIZE = len(bdd_oia_dataset.word_to_ind.keys())\n",
    "padded = nn.utils.rnn.pad_sequence(reason_batch,padding_value=NULL_INDEX)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "embedding_layer = nn.Embedding(num_embeddings=DICT_SIZE,embedding_dim=embedding_size)\n",
    "embedded = embedding_layer(padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([12, 10, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 6, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "image_affine_layers = nn.Sequential(nn.Linear(1280,128),\n",
    "                                    nn.ReLU()) \n",
    "\n",
    "image_f = image_affine_layers(image_feature.permute(0,2,3,1)).permute(0,3,1,2) # B , F , H , W \n",
    "image_f.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(prev_h, A):\n",
    "    \"\"\"\n",
    "    dot product between the hidden state and image embeddings\n",
    "    A: B,F,H,W\n",
    "    prev_h: B,F\n",
    "    \"\"\"\n",
    "    B,F,H,W = A.shape\n",
    "    attention_score = torch.bmm(prev_h.view(B,1,F),A.view(B,F,H*W)).squeeze(1).div(F**0.5) # B, H*W\n",
    "    attention_weights = nn.functional.softmax(attention_score,dim=1) # B, H*W\n",
    "\n",
    "    attended_features = torch.bmm(A.view(B,F,H*W), attention_weights.view(B,H*W,1)).squeeze(2) # B, F\n",
    "    \n",
    "    attention_weights = attention_weights.view(B,H,W)\n",
    "\n",
    "    return attention_weights, attended_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([10, 6, 10]), torch.Size([10, 128]))"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "image_feature_dim = 128\n",
    "hidden_dim = 128\n",
    "init_c_layer = nn.Linear(image_feature_dim,hidden_dim)\n",
    "init_h_layer = nn.Linear(image_feature_dim,hidden_dim)\n",
    "\n",
    "def init_hidden_state(A):\n",
    "    mean_image = A.mean(dim=(2,3)) # N,image_feature_dim\n",
    "    h0 = init_h_layer(mean_image)\n",
    "    c0 = init_c_layer(mean_image)\n",
    "    return h0, c0\n",
    "\n",
    "h0,c0 = init_hidden_state(image_f)\n",
    "\n",
    "attention_weights, attended_features = dot_product_attention(h0,image_f)\n",
    "attention_weights.shape,attended_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "torch.cat([attended_features,attended_features],axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9af40b6350>"
      ]
     },
     "metadata": {},
     "execution_count": 68
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"235.458125pt\" version=\"1.1\" viewBox=\"0 0 366.0625 235.458125\" width=\"366.0625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 235.458125 \nL 366.0625 235.458125 \nL 366.0625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 211.58 \nL 355.3625 211.58 \nL 355.3625 10.7 \nL 20.5625 10.7 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p93da8cb7ec)\">\n    <image height=\"201\" id=\"image7f0ecd43b3\" transform=\"scale(1 -1)translate(0 -201)\" width=\"335\" x=\"20.5625\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAU8AAADJCAYAAACjU7CDAAAABHNCSVQICAgIfAhkiAAABE5JREFUeJzt1rGKXAUYhuFvdmazu5LdBAKrRoIRCxUbSREUO6/BwhvyTsSbsLCwshBMEQtBxEowugazGeJuZsbCO/iafwLPcwUf5xze8y/uf/XlLsN2F0fTE5Ikdx4tpifk+O/t9IQkyenjP6cn5OqtW9MTkiTr8xvTE3J1c/7bTJLr0/kd/3xwPT0hSXIwPQDgVSSeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToLC6/d3x9Iac/LWdnpAkOfvmp+kJWX/63vSEJMnm51+mJyR3H0wvSJKc/raenpD13ZPpCUmSzclyesLenHx7MgPg1SKeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFFYv7iymN+Tio930hCTJkwcfTk/I4uX8+0iS8xsPpydke7gfz+LZvePpCcl2esD/dsvpBcny6Wp6QhKXJ0BFPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BhdfDw6fSGfHL++/SEJMn3v96fnpCbP5xMT0iSLP/dTU/I4eVmekKS5Oji5fSErF8/nJ6QJLm8N39vbY+30xOSuDwBKuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAYfXG2bPpDfn6nW+nJyRJvshn0xPy+NH70xOSJFdny+kJObjej3/7YrebnpDnb+7Hs7i6Nf8sdofzGxKXJ0BFPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BBPAEK4glQEE+AgngCFMQToCCeAAXxBCiIJ0BhNT0gST7+8fPpCUmSP56cTU9I3t5ML0iSXL67m56Q5fP9+Lcv1/M7Nq9tpyckSbZH899FlnuwIS5PgIp4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUBBPgIJ4AhTEE6AgngAF8QQoiCdAQTwBCuIJUPgPUApTLSaQ7FEAAAAASUVORK5CYII=\" y=\"-10.58\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m2d0a12bd19\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.3025\" xlink:href=\"#m2d0a12bd19\" y=\"211.58\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(34.12125 226.178438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.2625\" xlink:href=\"#m2d0a12bd19\" y=\"211.58\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(101.08125 226.178438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.2225\" xlink:href=\"#m2d0a12bd19\" y=\"211.58\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(168.04125 226.178438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.1825\" xlink:href=\"#m2d0a12bd19\" y=\"211.58\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(235.00125 226.178438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"305.1425\" xlink:href=\"#m2d0a12bd19\" y=\"211.58\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(301.96125 226.178438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m371cca1b52\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m371cca1b52\" y=\"27.44\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 31.239219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m371cca1b52\" y=\"60.92\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(7.2 64.719219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m371cca1b52\" y=\"94.4\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 98.199219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m371cca1b52\" y=\"127.88\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 131.679219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m371cca1b52\" y=\"161.36\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 165.159219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m371cca1b52\" y=\"194.84\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 198.639219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 211.58 \nL 20.5625 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 355.3625 211.58 \nL 355.3625 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 211.58 \nL 355.3625 211.58 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 10.7 \nL 355.3625 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p93da8cb7ec\">\n   <rect height=\"200.88\" width=\"334.8\" x=\"20.5625\" y=\"10.7\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(attention_weights[3,:,:].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "lstm_step = nn.LSTMCell(input_size=hidden_dim*2,hidden_size=hidden_dim)\n",
    "score_layer = nn.Linear(hidden_dim, DICT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'prob_layer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-7ba40f0d90af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlstm_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattended_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prob_layer' is not defined"
     ]
    }
   ],
   "source": [
    "max_seq_length = 33 \n",
    "hs = []\n",
    "scores = []\n",
    "attention_weights = []\n",
    "\n",
    "embedded_input = embedded[:-1,:,:]\n",
    "embedded_output = embedded[1:,:,:]\n",
    "\n",
    "T,B,E = embedded_output.shape\n",
    "h = h0\n",
    "c = c0\n",
    "for t in range(T):\n",
    "    attention_weight, attended_feature = dot_product_attention(h,image_f)\n",
    "    lstm_input = torch.cat([attended_features,embedded[t]],axis=1)\n",
    "    h,c = lstm_step(lstm_input,(h,c))\n",
    "    score = prob_layer(h)\n",
    "    hs.append(h)\n",
    "    scores.append(score)\n",
    "    attention_weights.append(attention_weight)\n",
    "\n",
    "hs, scores, attention_weights, = torch.stack(hs), torch.stack(scores) ,torch.stack(attention_weights) #hs: T B F, attention weights: T, B, H, W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([11, 10, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(ignore_index=NULL_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.6376, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "loss(scores.view(-1,DICT_SIZE),padded.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0842, 0.0628, 0.0739,  ..., 0.0633, 0.0718, 0.0813],\n",
       "        [0.0847, 0.0637, 0.0740,  ..., 0.0626, 0.0707, 0.0817],\n",
       "        [0.0856, 0.0631, 0.0736,  ..., 0.0612, 0.0726, 0.0778],\n",
       "        ...,\n",
       "        [0.0726, 0.0764, 0.0686,  ..., 0.0682, 0.0759, 0.0849],\n",
       "        [0.0699, 0.0779, 0.0670,  ..., 0.0693, 0.0756, 0.0873],\n",
       "        [0.0657, 0.0797, 0.0710,  ..., 0.0714, 0.0743, 0.0875]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "scores.view(-1,DICT_SIZE)"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder():\n",
    "    mobilenet = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "    encoder = nn.Sequential(*list(mobilenet.children())[:-1])\n",
    "    return encoder\n",
    "\n",
    "encoder = get_encoder()\n",
    "\n",
    "decoder = ReasonDecoder(image_f_dim=1280,\\\n",
    "                        embedding_dim=128, \\\n",
    "                        hidden_dim=128, \\\n",
    "                        dict_size=DICT_SIZE, \\\n",
    "                        device='cpu',\\\n",
    "                        null_index=NULL_INDEX, \\\n",
    "                        using_gate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./saved_models/soft_attention/soft_attention14.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['encoder', 'decoder', 'decoder_optimizer'])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ReasonDecoder(\n",
       "  (embedding_layer): Embedding(14, 128)\n",
       "  (image_affine_layers): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (init_c_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (init_h_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lstm_step): LSTMCell(256, 128)\n",
       "  (score_layer): Linear(in_features=128, out_features=14, bias=True)\n",
       "  (gate): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reason(image_feature, max_length=33):\n",
    "\n",
    "    h = h0\n",
    "    c = c0\n",
    "    hs = []\n",
    "    scores = []\n",
    "    attention_weights = []\n",
    "    \n",
    "    for t in range(max_length):\n",
    "        attention_weight, attended_features = decoder.dot_product_attention(h,image_f)\n",
    "        # print(attended_feature.shape)\n",
    "        if decoder.using_gate:\n",
    "            gate_val = decoder.gate(h)\n",
    "            # print(gate_val.shape)\n",
    "            attended_features = attended_features*gate_val\n",
    "        \n",
    "        lstm_input = torch.cat([attended_features,embedded[t]],axis=1)\n",
    "        h,c = decoder.lstm_step(lstm_input,(h,c))\n",
    "        score = decoder.score_layer(h)\n",
    "        hs.append(h)\n",
    "        scores.append(score)\n",
    "        attention_weights.append(attention_weight)\n",
    "    hs, scores, attention_weights = torch.stack(hs), torch.stack(scores) ,torch.stack(attention_weights)\n",
    "\n",
    "    return hs,scores, attention_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "image_f = decoder.image_affine_layers(image_feature.permute(0,2,3,1)).permute(0,3,1,2) # B , F , H , W \n",
    "h0,c0 = decoder.init_hidden_state(image_f)\n",
    "prev_word = torch.LongTensor([3]*10).to(device) # start tensor\n",
    "prev_embedding = decoder.embedding_layer(prev_word)\n",
    "prev_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h0\n",
    "c = c0\n",
    "hs = []\n",
    "scores = []\n",
    "attention_weights = []\n",
    "reasons = []\n",
    "max_length = 33\n",
    "for t in range(max_length):\n",
    "    prev_embedding = decoder.embedding_layer(prev_word)\n",
    "    attention_weight, attended_features = decoder.dot_product_attention(h,image_f)\n",
    "    # print(attended_feature.shape)\n",
    "    if decoder.using_gate:\n",
    "        gate_val = decoder.gate(h)\n",
    "        # print(gate_val.shape)\n",
    "        attended_features = attended_features*gate_val\n",
    "    \n",
    "    lstm_input = torch.cat([attended_features,prev_embedding],axis=1)\n",
    "    h,c = decoder.lstm_step(lstm_input,(h,c))\n",
    "    score = decoder.score_layer(h)\n",
    "    prev_word = torch.max(score,1)[1]\n",
    "    \n",
    "    hs.append(h)\n",
    "    scores.append(score)\n",
    "    attention_weights.append(attention_weight)\n",
    "    reasons.append(prev_word)\n",
    "hs, scores, attention_weights, reasons = torch.stack(hs), torch.stack(scores),torch.stack(attention_weights),torch.stack(reasons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([33, 10, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "reasons.shape\n",
    "hs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  3,  8,  3,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3],\n",
       "        [ 8,  8, 12,  3, 12,  3,  8,  8,  8,  3]])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 14])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([3.4582, 3.3726, 3.5543, 4.3924, 2.9804, 4.4230, 3.9803, 3.2614, 4.1859,\n",
       "        4.1699], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 3,  3,  3,  3, 12,  3,  3,  3,  3,  3]))"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "torch.max(score,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}